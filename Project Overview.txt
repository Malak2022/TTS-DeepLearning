ğŸ“– Project Description

This project implements a deep learning Text-to-Speech (TTS) system using Tacotron 2 as the core sequence-to-sequence model. The objective is to transform raw text input into natural-sounding human speech, leveraging neural network architectures for both text-to-mel spectrogram generation and waveform synthesis.

âš™ï¸ Model Workflow

Input: Raw text string (e.g., â€œHello, how are you?â€).

Preprocessing:

Convert text into a sequence of characters.

Apply text normalization and tokenization.

Encoding: Character sequences are processed with a bidirectional LSTM encoder.

Decoding with Attention:

The decoder, enhanced with an attention mechanism, predicts mel spectrograms.

Post-processing:

A convolutional postnet refines the generated spectrograms.

Waveform Reconstruction:

Initial synthesis with Griffin-Lim algorithm.

Optionally improved with neural vocoders (WaveGlow, HiFi-GAN).

Output:

Audio waveform

Visualizations (mel spectrograms, attention alignments)

ğŸ“Š Training Configuration

Dataset: Preprocessed LJSpeech corpus (13,100 samples).

Loss Functions:

L1 loss on mel spectrogram predictions

Gate prediction loss for sequence termination

Optimizer: Adam with learning rate scheduling.

Monitoring:

TensorBoard dashboards for loss and alignment tracking

Checkpointing: Automatic saving of best-performing models.

ğŸµ Audio Generation

Mel Spectrogram Prediction â†’ Griffin-Lim â†’ Audio waveform

Future Improvement: Replace Griffin-Lim with HiFi-GAN vocoder for higher fidelity.

ğŸ“ˆ Expected Outcomes

Training Duration: Several hours to days (depending on hardware resources).

Convergence Indicators:

Decreasing loss curves

Clear diagonal attention alignments

Audio Quality:

Good intelligibility

Natural prosody with sufficient training

ğŸ“‚ Project Structure
tacotron2/
â”œâ”€â”€ configs/config.py           # Configuration settings
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tacotron2.py            # Main Tacotron2 model
â”‚   â”œâ”€â”€ layers.py               # Encoder, Attention, Postnet layers
â”‚   â””â”€â”€ decoder.py              # Decoder with attention mechanism
â”œâ”€â”€ data/dataset.py             # Dataset class for preprocessed data
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                # Training loop
â”‚   â””â”€â”€ loss.py                 # Loss functions
â”œâ”€â”€ inference/synthesize.py     # Inference script
â””â”€â”€ utils/
    â”œâ”€â”€ text.py                 # Text preprocessing utilities
    â””â”€â”€ audio.py                # Audio processing utilities


Additional Scripts:

Training: train_tacotron2.py, train_tacotron2_fast.py

Synthesis: synthesize_text.py, quick_start.py

Testing & Evaluation: test_model.py, evaluate_model.py

Setup: setup_tacotron2.py

Alternative Vocoder Testing: test_with_better_vocoder.py

ğŸš€ Usage (Example Commands)
# 1. Setup verification
python setup_tacotron2.py  

# 2. Start training
python train_tacotron2.py  

# 3. Evaluate training
python evaluate_model.py  

# 4. Test synthesis
python test_synthesis.py  

# 5. Synthesize text (after training)
python synthesize_text.py  

# 6. Interactive mode
python quick_start.py  

# 7. Test with HiFi-GAN vocoder
python test_with_better_vocoder.py